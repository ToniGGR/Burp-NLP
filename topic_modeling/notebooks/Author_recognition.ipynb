{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c42591f-ec0c-4921-9f45-a195ad2c4128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699839d4-102f-4258-86f0-c6cfe1aade53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U accelerate\n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad8642b-8aed-4d57-ae9c-c2518446f2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode no.</th>\n",
       "      <th>speaker</th>\n",
       "      <th>dialouge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Rick</td>\n",
       "      <td>stumbl drunkenli turn light morty got tocom ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Morty</td>\n",
       "      <td>rub eye rick go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Rick</td>\n",
       "      <td>got surpris morty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Morty</td>\n",
       "      <td>middl night talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Rick</td>\n",
       "      <td>spill alcohol morty ’s bed come got surpris dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9613</th>\n",
       "      <td>51</td>\n",
       "      <td>Mr. Poopybutthole:</td>\n",
       "      <td>cuz want love thought felt becom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9614</th>\n",
       "      <td>51</td>\n",
       "      <td>Mr. Poopybutthole:</td>\n",
       "      <td>ever think horrifi peopl love would found tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9615</th>\n",
       "      <td>51</td>\n",
       "      <td>Mr. Poopybutthole:</td>\n",
       "      <td>wish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9616</th>\n",
       "      <td>51</td>\n",
       "      <td>Mr. Poopybutthole:</td>\n",
       "      <td>wish brave enough love back know mayb tri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9617</th>\n",
       "      <td>51</td>\n",
       "      <td>Mr. Poopybutthole:</td>\n",
       "      <td>much time think ooo eee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9618 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode no.             speaker  \\\n",
       "0               1                Rick   \n",
       "1               1               Morty   \n",
       "2               1                Rick   \n",
       "3               1               Morty   \n",
       "4               1                Rick   \n",
       "...           ...                 ...   \n",
       "9613           51  Mr. Poopybutthole:   \n",
       "9614           51  Mr. Poopybutthole:   \n",
       "9615           51  Mr. Poopybutthole:   \n",
       "9616           51  Mr. Poopybutthole:   \n",
       "9617           51  Mr. Poopybutthole:   \n",
       "\n",
       "                                               dialouge  \n",
       "0     stumbl drunkenli turn light morty got tocom ju...  \n",
       "1                                       rub eye rick go  \n",
       "2                                     got surpris morty  \n",
       "3                                      middl night talk  \n",
       "4     spill alcohol morty ’s bed come got surpris dr...  \n",
       "...                                                 ...  \n",
       "9613                   cuz want love thought felt becom  \n",
       "9614    ever think horrifi peopl love would found tr...  \n",
       "9615                                               wish  \n",
       "9616          wish brave enough love back know mayb tri  \n",
       "9617                            much time think ooo eee  \n",
       "\n",
       "[9618 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_RAW_DATA = \"/home/anton/Anton/Studium/DHBW /Semester_5/Text_Analysis/repo/Burg-NLP/data/rick_and_morty_master.csv\"\n",
    "PATH_TO_CLEAN_DATA = \"/home/anton/Anton/Studium/DHBW /Semester_5/Text_Analysis/repo/Burg-NLP/topic_modeling/data/rick_and_morty_master_clean.csv\"\n",
    "\n",
    "df = pd.read_csv( PATH_TO_CLEAN_DATA , index_col = \"Unnamed: 0\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2451a33f-35e0-44a7-998f-5cb06a67b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d2717956-a7af-4048-a2ce-d45c50f87cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode no.</th>\n",
       "      <th>speaker</th>\n",
       "      <th>dialouge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rick</td>\n",
       "      <td>stumbl drunkenli turn light morty got tocom ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>morty</td>\n",
       "      <td>rub eye rick go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>rick</td>\n",
       "      <td>got surpris morty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>morty</td>\n",
       "      <td>middl night talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rick</td>\n",
       "      <td>spill alcohol morty s bed come got surpris dra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9357</th>\n",
       "      <td>49</td>\n",
       "      <td>rick</td>\n",
       "      <td>morty m tri stay humbl right soon guy gone po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9359</th>\n",
       "      <td>49</td>\n",
       "      <td>rick</td>\n",
       "      <td>bye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9361</th>\n",
       "      <td>49</td>\n",
       "      <td>rick</td>\n",
       "      <td>okay bring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9363</th>\n",
       "      <td>49</td>\n",
       "      <td>rick</td>\n",
       "      <td>excus pick name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9365</th>\n",
       "      <td>49</td>\n",
       "      <td>rick</td>\n",
       "      <td>jesus fucking christ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2841 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode no. speaker                                           dialouge\n",
       "0               1    rick  stumbl drunkenli turn light morty got tocom ju...\n",
       "1               1   morty                                    rub eye rick go\n",
       "2               1    rick                                  got surpris morty\n",
       "3               1   morty                                   middl night talk\n",
       "4               1    rick  spill alcohol morty s bed come got surpris dra...\n",
       "...           ...     ...                                                ...\n",
       "9357           49    rick   morty m tri stay humbl right soon guy gone po...\n",
       "9359           49    rick                                                bye\n",
       "9361           49    rick                                         okay bring\n",
       "9363           49    rick                                    excus pick name\n",
       "9365           49    rick                               jesus fucking christ\n",
       "\n",
       "[2841 rows x 3 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"speaker\"] = df[\"speaker\"].astype(\"str\")\n",
    "df['speaker'] = df[\"speaker\"].apply(lambda x : x.lower())\n",
    "df[\"dialouge\"] = df[\"dialouge\"].str.replace(\"[^\\w\\s]\",\"\" , regex=True)\n",
    "df[\"speaker\"] = df[\"speaker\"].str.replace(\"[^\\w\\s]\",\"\" , regex=True)\n",
    "speakers = [\"rick\" , \"morty\" , \"jerry\" , \"beth\"]\n",
    "df = df[df.speaker.isin(speakers)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "82f82990-36b3-433c-890c-e5bb42aca72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 0., 0.],\n",
       "       [1., 1., 1., ..., 1., 0., 0.],\n",
       "       [1., 1., 1., ..., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel = MultiLabelBinarizer()\n",
    "\n",
    "labels = multilabel.fit_transform(df['speaker']).astype('float32')\n",
    "\n",
    "texts = [ str(x).lower() for x in df['dialouge']]\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98ad212-fff3-47a4-8b4e-3b80f1a3cda2",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1ea9b368-749f-43e3-be42-cbd590ba69e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, AutoTokenizer\n",
    "from transformers import DistilBertForSequenceClassification, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d745716-e9df-4dbe-b76d-132f231c20a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels,\n",
    "                                                                    test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "84ff26e9-ed79-4e59-9207-2e2e956d4c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5557a66-a4ea-4411-9658-0467dfdc1fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets build custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "    self.texts = texts\n",
    "    self.labels = labels\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    text = str(self.texts[idx])\n",
    "    label = torch.tensor(self.labels[idx])\n",
    "\n",
    "    encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors='pt')\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoding['input_ids'].flatten(),\n",
    "        'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        'labels': label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9c9e1642-fa28-4dd0-b560-4a40c70479ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = CustomDataset(val_texts, val_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c1a71ada-429e-4aeb-8380-f7e09ec8a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2306e6af-228b-48dc-a683-d32390f0ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    output_dir = './results',\n",
    "    num_train_epochs=5,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2\n",
    ")\n",
    "model.to(\"cpu\")\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset = val_dataset,\n",
    "                  compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "18d828f8-c8f1-4fb5-8ecf-abe604eec89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='1420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   6/1420 00:06 < 39:03, 0.60 it/s, Epoch 0.02/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Anton/Studium/DHBW /Semester_5/Text_Analysis/repo/Burg-NLP/venv/lib/python3.10/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Anton/Studium/DHBW /Semester_5/Text_Analysis/repo/Burg-NLP/venv/lib/python3.10/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Anton/Studium/DHBW /Semester_5/Text_Analysis/repo/Burg-NLP/venv/lib/python3.10/site-packages/transformers/trainer.py:3712\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_accepts_loss_kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3710\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m-> 3712\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3714\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/Anton/Studium/DHBW /Semester_5/Text_Analysis/repo/Burg-NLP/venv/lib/python3.10/site-packages/accelerate/accelerator.py:2246\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2246\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Anton/Studium/DHBW /Semester_5/Text_Analysis/repo/Burg-NLP/venv/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Anton/Studium/DHBW /Semester_5/Text_Analysis/repo/Burg-NLP/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Anton/Studium/DHBW /Semester_5/Text_Analysis/repo/Burg-NLP/venv/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
